#!/usr/bin/env node
/**
 * Unified MCP Docs Engine (Vectra-backed)
 * - Web docs indexing (HTML â†’ text â†’ chunks)
 * - Project indexing (AST â†’ symbols â†’ structured chunks)
 * - Vector-based semantic search (Vectra)
 * - Separate indexes per engineId: web | project
 * - High-precision context builder for LLMs
 * - Incremental autorefresh via chokidar
 */
import { serviceLog } from "./logger.js";

function stringify(args) {
  return args.map((a) =>
    typeof a === "string" ? a : JSON.stringify(a, null, 2)
  );
}
console.log = (...args) => {
  serviceLog("[LOG]", ...stringify(args));
};
console.warn = (...args) => {
  serviceLog("[WARN]", ...stringify(args));
};
console.error = (...args) => {
  serviceLog("[ERROR]", ...stringify(args));
};
process.on("uncaughtException", (err) => {
  serviceLog("[FATAL] uncaughtException", err?.stack || err);
});

process.on("unhandledRejection", (reason) => {
  serviceLog("[FATAL] unhandledRejection", reason);
});
import fs from "node:fs/promises";
import { chunkText } from "./util/chunkText.js";
import { mmrSelect } from "./util/mmrSelect.js";
import { expandByGraph } from "./util/expandByGraph.js";
import { loadWebSource } from "./util/loadWebSource.js";
import { extractProjectSymbols } from "./util/extractProjectSymbols.js";
import {
  cleanText,
  hashText,
  estimateTokens,
  detectSection,
  TOKEN_ESTIMATE,
} from "./util/text.js";
import { embed } from "./util/embed.js";
import path from "node:path";
import glob from "fast-glob";
import chokidar from "chokidar";
import { LocalIndex } from "vectra";
import { fileURLToPath } from "node:url";
//cbcvb
const __filename = fileURLToPath(import.meta.url);
const __dirname = path.dirname(__filename);
/* ===================== CONFIG ===================== */
const ROOT = path.resolve(__dirname, "..");
const INDEX_ROOT = path.join(ROOT, ".docs-index");
const ENGINES = {
  web: {
    sourcesFile: path.join(ROOT, "docs_sources.json"),
    indexDir: "web",
    chunkDefaults: { size: 3500, overlap: 200 },
  },
  project: {
    sourcesFile: path.join(ROOT, "docs_project.json"),
    indexDir: "project",
    chunkDefaults: { size: 2500, overlap: 200 },
  },
};

const SEARCH_CONFIG = {
  topK: 12,
};

const PERSIST_FILES = {
  projectFileIndex: path.join(
    INDEX_ROOT,
    ENGINES.project.indexDir,
    "project-file-index.json"
  ),
};

const GRAPH_FILE = path.join(
  INDEX_ROOT,
  ENGINES.project.indexDir,
  "graph.json"
);
const projectGraph = new Map(); // file -> { imports, exports }
const pageRank = new Map(); // file -> number
const reverseProjectGraph = new Map(); // file -> Set(importers)
const fileVersions = new Map();
const projectFileIndex = new Map();
const engineLocks = new Map(); // engineId -> Promise queue
const engineState = new Map();
/* ===================== VECTOR INDEX ===================== */
const vectorIndexes = {};
const vectorIndexInit = {};

//âŒ Ð£Ð‘Ð ÐÐ¢Ð¬ Ð¸Ð· getVectorIndex Ð»ÑŽÐ±ÑƒÑŽ Ð±Ð¸Ð·Ð½ÐµÑ-Ð»Ð¾Ð³Ð¸ÐºÑƒ getVectorIndex Ñ‚Ð¾Ð»ÑŒÐºÐ¾: Ð¾Ñ‚ÐºÑ€Ñ‹Ñ‚ÑŒ Ð¸Ð½Ð´ÐµÐºÑ Ð²ÐµÑ€Ð½ÑƒÑ‚ÑŒ instance â— ÐÐ˜ÐšÐÐšÐ˜Ð¥: watcher scan graph persist
async function getVectorIndex(engineId) {
  if (vectorIndexes[engineId]) {
    return vectorIndexes[engineId];
  }

  const dir = path.join(INDEX_ROOT, ENGINES[engineId].indexDir, "vectra");

  const idx = new LocalIndex(dir);
  vectorIndexes[engineId] = idx;
  return idx;
}

/* ===================== STORAGE ==================== */
async function ensureEngineStore(engineId) {
  const dir = path.join(INDEX_ROOT, ENGINES[engineId].indexDir);
  const vectraDir = path.join(dir, "vectra");
  try {
    await fs.mkdir(vectraDir, { recursive: true }); // ÑÐ¾Ð·Ð´Ð°ÑÑ‚ Ð¸ Ñ€Ð¾Ð´Ð¸Ñ‚ÐµÐ»Ñ, ÐµÑÐ»Ð¸ Ð½ÑƒÐ¶Ð½Ð¾
    console.error("[ensureEngineStore] ensured", { dir, vectraDir });
  } catch (e) {
    console.error("[ensureEngineStore] mkdir failed", {
      dir,
      vectraDir,
      err: e.stack || e,
    });
    throw e;
  }
}

/* ===================== UTILS ===================== */
async function indexWeb(cfg) {
  if (engineState.get("web") !== "ready") {
    throw new Error("web engine not initialized");
  }

  const index = await getVectorIndex("web");

  const visited = new Set();

  for (const def of Object.values(cfg.web_recurses || {})) {
    const { url, depth = 1, include, exclude, maxPages = 100 } = def;

    await walkWeb(
      url,
      depth,
      { include, exclude, maxPages },
      async (pageUrl) => {
        if (visited.has(pageUrl)) return;
        visited.add(pageUrl);
        serviceLog("[WEB] load", pageUrl);
        const text = await loadWebSource(pageUrl);
        if (!text || text.length < 50) return;
        const chunks = chunkText(text, 800, 200);
        for (let i = 0; i < chunks.length; i++) {
          const raw = chunks[i];
          // âœ… 1. ÐºÐ°Ð½Ð¾Ð½Ð¸Ð·Ð°Ñ†Ð¸Ñ
          const cleaned = cleanText(raw);
          if (!cleaned) continue;
          // âœ… 2. ÑÐµÐºÑ†Ð¸Ñ â€” Ð¢ÐžÐ›Ð¬ÐšÐž Ð¾Ñ‚ cleaned
          const section = detectSection(cleaned);
          // âœ… 3. embedding â€” Ð¢ÐžÐ›Ð¬ÐšÐž Ð¾Ñ‚ cleaned
          const vector = await embedQueued(cleaned);
          await upsertVectorItem(index, "web", {
            id: `web:${pageUrl}#${i}`,
            vector,
            content: cleaned, // â¬…ï¸ ÐšÐ Ð˜Ð¢Ð˜Ð§ÐÐž
            metadata: {
              engineId: "web",
              source: pageUrl,
              section,
              order: i,
            },
          });
        }
      }
    );
  }

  serviceLog("[WEB] indexing complete", {
    pages: visited.size,
  });
}
// initEngineStrict â€” Ð•Ð”Ð˜ÐÐ¡Ð¢Ð’Ð•ÐÐÐÐ¯ Ñ‚Ð¾Ñ‡ÐºÐ° Ð¸Ð½Ð¸Ñ†Ð¸Ð°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ð¸
async function initEngineStrict(engineId) {
  serviceLog("[INIT] engine start", engineId);

  const engineDir = path.join(INDEX_ROOT, ENGINES[engineId].indexDir);
  const vectraDir = path.join(engineDir, "vectra");

  // 1ï¸âƒ£ Ð´Ð¸Ñ€ÐµÐºÑ‚Ð¾Ñ€Ð¸Ð¸ (Ð–ÐÐ¡Ð¢ÐšÐž)
  await fs.mkdir(vectraDir, { recursive: true });

  // 2ï¸âƒ£ Ð¿Ð¾Ð»ÑƒÑ‡Ð°ÐµÐ¼ index (Ð‘Ð•Ð— Ð¿Ð¾Ð±Ð¾Ñ‡ÐµÐº)
  const index = await getVectorIndex(engineId);

  // 3ï¸âƒ£ Ð¿Ñ€Ð¾Ð²ÐµÑ€ÑÐµÐ¼ ÑÐ¾Ð·Ð´Ð°Ð½ Ð»Ð¸ Ð¸Ð½Ð´ÐµÐºÑ
  let created = false;
  try {
    created = await index.isIndexCreated();
  } catch (e) {
    serviceLog("[INIT] isIndexCreated failed", e);
    created = false;
  }

  // 4ï¸âƒ£ ÐµÑÐ»Ð¸ Ð½ÐµÑ‚ â€” ÑÐ¾Ð·Ð´Ð°Ñ‘Ð¼
  if (!created) {
    serviceLog("[INIT] createIndex", engineId);
    await index.createIndex();
  }

  // ðŸ’£ Ð“ÐÐ ÐÐÐ¢Ð˜Ð¯: index.json Ð±ÑƒÐ´ÐµÑ‚ ÑÐ¾Ð·Ð´Ð°Ð½
  await index.insertItem({
    id: "__init__",
    vector: new Array(256).fill(0),
    content: "init",
  });
  await index.deleteItem("__init__");
  // 6ï¸âƒ£ Ð¿Ð¾Ð¼ÐµÑ‡Ð°ÐµÐ¼ ÑÐ¾ÑÑ‚Ð¾ÑÐ½Ð¸Ðµ
  engineState.set(engineId, "ready");

  serviceLog("[INIT] engine ready", engineId);
}

function cleanupProjectFileState(filePath) {
  projectGraph.delete(filePath);
  reverseProjectGraph.delete(filePath);
  removeReverseEdges(filePath);
  projectFileIndex.delete(filePath);
  schedulePersistSave();
  scheduleGraphSave();
}

function computeGraphScore(hit, seedFiles) {
  if (seedFiles.includes(hit.source)) return 1.0;

  const node = projectGraph.get(hit.source);
  if (!node) return 0.6;

  // Ð¿Ñ€ÑÐ¼Ð¾Ð¹ Ð¸Ð¼Ð¿Ð¾Ñ€Ñ‚ Ð¾Ñ‚ seed
  if (node.imports?.some((i) => seedFiles.includes(i))) {
    return 0.8;
  }

  // seed Ð¸Ð¼Ð¿Ð¾Ñ€Ñ‚Ð¸Ñ€ÑƒÐµÑ‚ ÑÑ‚Ð¾Ñ‚ Ñ„Ð°Ð¹Ð»
  for (const seed of seedFiles) {
    const seedNode = projectGraph.get(seed);
    if (seedNode?.imports?.includes(hit.source)) {
      return 0.8;
    }
  }

  return 0.6;
}

/* ===================== PageRank ===================== */
function recomputePageRank({ damping = 0.85, iterations = 20 } = {}) {
  const files = [...projectGraph.keys()];
  const N = files.length;
  if (!N) return;

  // Ð½Ð°Ñ‡Ð°Ð»ÑŒÐ½Ð¾Ðµ Ð·Ð½Ð°Ñ‡ÐµÐ½Ð¸Ðµ
  const init = 1 / N;
  files.forEach((f) => pageRank.set(f, init));

  // Ð¿Ñ€ÐµÐ´Ñ€Ð°ÑÑ‡Ñ‘Ñ‚: file -> importers[]
  const incoming = new Map();
  for (const [file, node] of projectGraph.entries()) {
    for (const imp of node.imports || []) {
      if (!incoming.has(imp)) incoming.set(imp, []);
      incoming.get(imp).push(file);
    }
  }

  // Ð¸Ñ‚ÐµÑ€Ð°Ñ†Ð¸Ð¸
  for (let i = 0; i < iterations; i++) {
    const next = new Map();

    // Ð¿Ñ€ÐµÐ´Ð²Ð°Ñ€Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ð¾ Ð¿Ð¾ÑÑ‡Ð¸Ñ‚Ð°ÐµÐ¼ ÑÑƒÐ¼Ð¼Ñƒ Ñ€Ð°Ð½Ð³Ð¾Ð² "dangling" (ÑƒÐ·Ð»Ñ‹ Ñ Ð½ÑƒÐ»ÐµÐ²Ð¾Ð¹ Ð¸ÑÑ…Ð¾Ð´ÑÑ‰ÐµÐ¹ ÑÑ‚ÐµÐ¿ÐµÐ½ÑŒÑŽ)
    let danglingSum = 0;
    for (const f of files) {
      const outDeg = projectGraph.get(f)?.imports?.length ?? 0;
      if (outDeg === 0) {
        danglingSum += pageRank.get(f) ?? 0;
      }
    }

    for (const file of files) {
      let sum = 0;

      const importers = incoming.get(file) || [];
      for (const other of importers) {
        const outDegree = projectGraph.get(other)?.imports?.length ?? 0;
        if (outDegree > 0) {
          sum += (pageRank.get(other) ?? 0) / outDegree;
        }
        // ÐµÑÐ»Ð¸ outDegree == 0, Ð²ÐºÐ»Ð°Ð´ ÑƒÑ…Ð¾Ð´Ð¸Ñ‚ Ð² danglingSum Ð¸ Ð±ÑƒÐ´ÐµÑ‚ Ñ€Ð°ÑÐ¿Ñ€ÐµÐ´ÐµÐ»Ñ‘Ð½ Ð¾Ñ‚Ð´ÐµÐ»ÑŒÐ½Ð¾
      }

      // Ñ€Ð°ÑÐ¿Ñ€ÐµÐ´ÐµÐ»ÑÐµÐ¼ dangling mass Ñ€Ð°Ð²Ð½Ð¾Ð¼ÐµÑ€Ð½Ð¾
      const danglingContribution = danglingSum / N;

      const rank = (1 - damping) / N + damping * (sum + danglingContribution);
      next.set(file, rank);
    }

    // Ð·Ð°Ð¿Ð¸ÑÑ‹Ð²Ð°ÐµÐ¼ Ð¾Ð±Ñ€Ð°Ñ‚Ð½Ð¾
    pageRank.clear();
    for (const [f, r] of next.entries()) {
      pageRank.set(f, r);
    }
  }
}
/* ===================== PERSIST ===================== */
let graphPersistTimer = null;

function scheduleGraphSave(delay = 1000) {
  if (graphPersistTimer) clearTimeout(graphPersistTimer);
  graphPersistTimer = setTimeout(() => {
    try {
      recomputePageRank();
      saveProjectGraph();
    } catch (e) {
      console.warn("[graph] save failed:", e.message);
    }
  }, delay);
}
//++++++++ Ñ€Ð°Ð±Ð¾Ñ‚Ð°ÐµÐ¼ Ñ Ð³Ñ€Ð°Ñ„Ð¾Ð¼ - Ð·Ð°Ð´Ð°Ñ‡Ð° Ð¾Ð±Ð½Ð°Ñ€ÑƒÐ¶Ð¸Ñ‚ÑŒ ÐºÑƒÐ´Ð° ÑƒÑ…Ð¾Ð´ÑÑ‚ ÑÐºÑÐ¿Ð¾Ñ€Ñ‚Ñ‹++++++++++
function addReverseEdge(from, to) {
  if (!reverseProjectGraph.has(to)) {
    reverseProjectGraph.set(to, new Set());
  }
  reverseProjectGraph.get(to).add(from);
}

//++++++++ Ñ€Ð°Ð±Ð¾Ñ‚Ð°ÐµÐ¼ Ñ Ð³Ñ€Ð°Ñ„Ð¾Ð¼ - Ð·Ð°Ð´Ð°Ñ‡Ð° Ð¾Ð±Ð½Ð°Ñ€ÑƒÐ¶Ð¸Ñ‚ÑŒ ÐºÑƒÐ´Ð° ÑƒÑ…Ð¾Ð´ÑÑ‚ ÑÐºÑÐ¿Ð¾Ñ€Ñ‚Ñ‹++++++++++
function removeReverseEdges(file) {
  for (const set of reverseProjectGraph.values()) {
    set.delete(file);
  }
}

async function loadProjectGraph() {
  try {
    const raw = await fs.readFile(GRAPH_FILE, "utf8");
    const json = JSON.parse(raw);
    projectGraph.clear();
    reverseProjectGraph.clear();

    for (const [file, data] of Object.entries(json)) {
      projectGraph.set(file, {
        imports: data.imports || [],
        exports: data.exports || [],
      });
    }
    // rebuild reverseProjectGraph
    for (const [file, data] of projectGraph.entries()) {
      for (const imp of data.imports || []) {
        addReverseEdge(file, imp);
      }
    }
    // recompute pageRank after loading the graph so consumers get correct values
    recomputePageRank();
  } catch (e) {
    if (e.code !== "ENOENT") {
      console.warn("[graph] load failed:", e.message);
    }
  }
}

async function saveProjectGraph() {
  const json = {};
  for (const [file, data] of projectGraph.entries()) {
    json[file] = data;
  }

  await fs.mkdir(path.dirname(GRAPH_FILE), { recursive: true });

  const tmp = GRAPH_FILE + ".tmp";
  await fs.writeFile(tmp, JSON.stringify(json, null, 2));
  await fs.rename(tmp, GRAPH_FILE);
}

/* ðŸ”§ FIX: debounce + atomic persist, Ñ‡Ñ‚Ð¾Ð±Ñ‹ Ð¸Ð·Ð±ÐµÐ¶Ð°Ñ‚ÑŒ Ð³Ð¾Ð½Ð¾Ðº */
let persistTimer = null;

function schedulePersistSave(delay = 1000) {
  if (persistTimer) clearTimeout(persistTimer);
  persistTimer = setTimeout(() => {
    saveProjectFileIndex().catch((e) =>
      console.warn("[persist] save failed:", e.message)
    );
  }, delay);
}

async function loadProjectFileIndex() {
  try {
    const raw = await fs.readFile(PERSIST_FILES.projectFileIndex, "utf8");
    const data = JSON.parse(raw);
    projectFileIndex.clear();
    for (const [file, hashes] of Object.entries(data)) {
      projectFileIndex.set(file, new Set(hashes));
    }
  } catch (e) {
    if (e.code !== "ENOENT") {
      console.warn("[project-index] persist load failed:", e.message);
    }
  }
}

async function saveProjectFileIndex() {
  const data = {};
  for (const [file, hashes] of projectFileIndex.entries()) {
    data[file] = [...hashes];
  }

  await fs.mkdir(path.dirname(PERSIST_FILES.projectFileIndex), {
    recursive: true,
  });

  const tmp = PERSIST_FILES.projectFileIndex + ".tmp";
  await fs.writeFile(tmp, JSON.stringify(data, null, 2), "utf8");
  await fs.rename(tmp, PERSIST_FILES.projectFileIndex); // atomic
}

/* ===================== UPSERT ===================== */

async function withEngineLock(engineId, fn) {
  const prev = engineLocks.get(engineId) || Promise.resolve();
  let release;
  const next = new Promise((r) => (release = r));
  engineLocks.set(
    engineId,
    prev.then(() => next)
  );

  try {
    await prev;
    return await fn();
  } finally {
    release();
    if (engineLocks.get(engineId) === next) {
      engineLocks.delete(engineId);
    }
  }
}
//hjg
async function upsertVectorItem(index, engineId, item) {
  console.log("[VECTRA UPSERT]", engineId, item.id, item.vector.length);
  return await withEngineLock(engineId, async () => {
    try {
      await index.deleteItem(item.id);
    } catch {}
    await index.insertItem(item);
  });
}

/* ===================== INCREMENTAL PROJECT INDEX ===================== */

async function removeFileFromIndex(index, filePath) {
  const hashes = projectFileIndex.get(filePath);
  if (!hashes) return;

  for (const h of hashes) {
    try {
      await index.deleteItem(h);
    } catch {}
  }

  projectFileIndex.delete(filePath);
  schedulePersistSave(); // ðŸ”§ FIX
}
async function indexProject(cfg) {
  const roots = Object.values(cfg.local_recurses || {}).map((p) =>
    path.resolve(ROOT, p.root)
  );
  // 1ï¸âƒ£ initial scan
  for (const absRoot of roots) {
    const p = Object.values(cfg.local_recurses).find(
      (x) => path.resolve(ROOT, x.root) === absRoot
    );
    if (!p) continue;

    const files = await glob(p.include, {
      cwd: absRoot,
      ignore: p.exclude,
      absolute: true,
    });

    for (const absFile of files) {
      if (!VALID_EXT.test(absFile)) continue;
      const rel = path.relative(ROOT, absFile).replace(/\\/g, "/");
      await indexProjectFile(rel, ROOT);
    }
  }
  // 2ï¸âƒ£ watcher â€” Ð¢ÐžÐ›Ð¬ÐšÐž ÐŸÐžÐ¡Ð›Ð• Ð£Ð¡ÐŸÐ•Ð¨ÐÐžÐ“Ðž Ð¡ÐšÐÐÐ
  watchProjectIndex(roots);
}
async function indexProjectFile(filePath, root) {
  if (engineState.get("project") !== "ready") {
    throw new Error("Engine not ready");
  }
  const version = Date.now();
  fileVersions.set(filePath, version);
  const index = await getVectorIndex("project");
  if (!index || !(await index.isIndexCreated())) {
    throw new Error("Vectra index not ready");
  }
  const abs = path.join(root, filePath);

  await removeFileFromIndex(index, filePath);

  let code;
  try {
    code = await fs.readFile(abs, "utf8");
  } catch {
    return;
  }

  let symbols;
  let result;
  try {
    result = extractProjectSymbols(code, filePath, root);
    symbols = result.chunks;
    // ðŸ”— Ð¾Ð±Ð½Ð¾Ð²Ð»ÑÐµÐ¼ Ð³Ñ€Ð°Ñ„
    removeReverseEdges(filePath);

    // forward graph
    projectGraph.set(filePath, {
      imports: result.imports,
      exports: result.exports,
    });

    // reverse graph
    for (const imp of result.imports) {
      addReverseEdge(filePath, imp);
    }

    scheduleGraphSave();
  } catch (e) {
    console.warn(
      "[project-index] skip file:",
      filePath,
      "\n",
      e?.message,
      "\n",
      e?.stack
    );
    return;
  }

  const hashes = new Set();

  /* ðŸ”§ FIX: batching embeddings Ð½Ð° ÑƒÑ€Ð¾Ð²ÐµÐ½ÑŒ Ñ„Ð°Ð¹Ð»Ð° */
  const existing = new Set((await index.listItems()).map((it) => it.id));
  for (let i = 0; i < symbols.length; i++) {
    const s = symbols[i];

    // âœ… 1. ÐºÐ°Ð½Ð¾Ð½Ð¸Ð·Ð°Ñ†Ð¸Ñ
    const cleaned = cleanText(s.text);
    if (!cleaned) continue;

    // âœ… 2. Ð»Ð¸Ð¼Ð¸Ñ‚
    const estimatedTokens = estimateTokens(cleaned);
    if (estimatedTokens > TOKEN_ESTIMATE.maxBudget) continue;

    // âœ… 3. ÑÑ‚Ð°Ð±Ð¸Ð»ÑŒÐ½Ñ‹Ð¹ hash
    const hash = hashText(cleaned);
    hashes.add(hash);

    if (existing.has(hash)) continue;

    // âœ… 4. embedding Ð¢ÐžÐ›Ð¬ÐšÐž Ð¾Ñ‚ cleaned
    const vector = await embedQueued(cleaned);

    await upsertVectorItem(index, "project", {
      id: hash,
      vector,
      content: cleaned, // â¬…ï¸ Ð’ÐÐ–ÐÐž
      metadata: {
        engineId: "project",
        source: filePath,
        section: s.section ?? detectSection(cleaned),
        estimatedTokens,
        imports: result.imports,
        exports: result.exports,
      },
    });
  }

  if (fileVersions.get(filePath) !== version) {
    fileVersions.delete(filePath);
    return;
  } // â— Ð¾Ñ‚Ð¼ÐµÐ½Ð°
  projectFileIndex.set(filePath, hashes);
  schedulePersistSave(); // ðŸ”§ FIX
}

async function removeProjectFile(filePath) {
  const index = await getVectorIndex("project");

  await removeFileFromIndex(index, filePath); // vectra
  cleanupProjectFileState(filePath); // graph + persist

  fileVersions.delete(filePath);
}

/* ===================== EMBEDDING QUEUE ===================== */

const EMBED_CONCURRENCY = 2;

// Ð¿Ñ€Ð¾ÑÑ‚Ð°Ñ Ð¾Ñ‡ÐµÑ€ÐµÐ´ÑŒ: Ð¼Ð°ÐºÑÐ¸Ð¼ÑƒÐ¼ EMBED_CONCURRENCY embed() Ð¾Ð´Ð½Ð¾Ð²Ñ€ÐµÐ¼ÐµÐ½Ð½Ð¾
let activeEmbeds = 0;
const embedWaiters = [];

async function embedQueued(text) {
  if (activeEmbeds >= EMBED_CONCURRENCY) {
    await new Promise((resolve) => embedWaiters.push(resolve));
  }

  activeEmbeds++;

  try {
    return await embed(text);
  } finally {
    activeEmbeds--;
    const next = embedWaiters.shift();
    if (next) next();
  }
}

/* ===================== SEARCH ===================== */

async function search(engineId, query, topK = SEARCH_CONFIG.topK) {
  if (engineState.get("project") !== "ready") {
    throw new Error("Engine not ready");
  }
  const index = await getVectorIndex(engineId);
  const queryVec = await embedQueued(query);
  // Ð±ÐµÑ€ÐµÐ¼ Ð±Ð¾Ð»ÑŒÑˆÐµ ÐºÐ°Ð½Ð´Ð¸Ð´Ð°Ñ‚Ð¾Ð²
  const raw = await index.queryItems(queryVec, topK * 3);
  return {
    queryVec,
    candidates: raw.map((r) => ({
      id: r.item.id,
      score: r.score,
      finalScore: r.score,
      vector: r.item.vector,
      text: r.item.content,
      source: r.item.metadata.source,
      section: r.item.metadata.section,
      estimatedTokens: r.item.metadata.estimatedTokens,
    })),
  };
}
//hgjhj
/* ===================== INDEXING ===================== */
let watcher = null;
// indexEngine â€” ÑÑ‚Ñ€Ð¾Ð³Ð°Ñ Ñ„Ð°Ð·Ð¾Ð²Ð°Ñ Ð¼Ð¾Ð´ÐµÐ»ÑŒ ÐÐ•Ð›Ð¬Ð—Ð¯ Ð·Ð°Ð¿ÑƒÑÐºÐ°Ñ‚ÑŒ watcher Ð´Ð¾ init, Ñ‡Ð¸Ñ‚Ð°Ñ‚ÑŒ cfg Ð´Ð¾ init, Ð´ÐµÐ»Ð°Ñ‚ÑŒ scan ÐµÑÐ»Ð¸ init ÑƒÐ¿Ð°Ð»
export async function indexEngine(engineId) {
  // PHASE 1 â€” HARD INIT
  await initEngineStrict(engineId); // â›” ÐµÑÐ»Ð¸ ÑƒÐ¿Ð°Ð»Ð¾ â€” ÑÑ‚Ð¾Ð¿

  // PHASE 2 â€” LOAD STATE
  if (engineId === "project") {
    await loadProjectGraph();
    await loadProjectFileIndex();
  }
  // PHASE 3 â€” INITIAL SCAN
  const cfg = JSON.parse(
    await fs.readFile(ENGINES[engineId].sourcesFile, "utf8")
  );

  if (engineId === "web") {
    await indexWeb(cfg);
  }

  if (engineId === "project") {
    await indexProject(cfg);
  }

  serviceLog("[ENGINE] ready", engineId);
}

/* ===================== REFRESH ===================== */
async function refresh(engineId) {
  await indexEngine(engineId);
}
/* ===================== CONTEXT BUILDER ===================== */
/**
 * v3 â€” architecture-aware buildContext
 * - group by source file
 * - exports â†’ imports â†’ symbols
 */
async function buildContext(engineId, query, budget) {
  //ÐµÑÐ»Ð¸ pageRank Ð¿ÑƒÑÑ‚Ð¾Ð¹ Ñ‚Ð¾ Ð·Ð°Ñ‰Ð¸Ñ‚Ð°
  if (!pageRank.size) {
    recomputePageRank();
  }
  // 1ï¸âƒ£ semantic seed
  const { queryVec, candidates } = await search(engineId, query, 20);
  const seedFilesCandidates = [...new Set(candidates.map((c) => c.source))];

  for (const c of candidates) {
    c.graphScore = computeGraphScore(c, seedFilesCandidates);
  }
  // MMR Ð¾Ñ‚Ð±Ð¾Ñ€
  const seedHits = mmrSelect(
    candidates,
    queryVec,
    10, // ÑÐºÐ¾Ð»ÑŒÐºÐ¾ Ñ€ÐµÐ°Ð»ÑŒÐ½Ð¾ Ñ…Ð¾Ñ‚Ð¸Ð¼
    0.7 // Ð±Ð°Ð»Ð°Ð½Ñ Ñ€ÐµÐ»ÐµÐ²Ð°Ð½Ñ‚Ð½Ð¾ÑÑ‚ÑŒ / Ñ€Ð°Ð·Ð½Ð¾Ð¾Ð±Ñ€Ð°Ð·Ð¸Ðµ
  );
  // ðŸŽ¯ graph-aware scoring
  const seedFiles = [...new Set(seedHits.map((h) => h.source))];

  for (const h of seedHits) {
    h.graphScore = computeGraphScore(h, seedFiles);
    const pr = pageRank.get(h.source) ?? 0.0001;

    h.finalScore = h.score * h.graphScore * Math.log(1 + pr * 10);
  }

  // 3ï¸âƒ£ graph expansion
  const expandedFiles = expandByGraph(seedFiles, 1);

  const index = await getVectorIndex(engineId);

  const expandedHits = [...seedHits];

  // 4ï¸âƒ£ Ð´Ð¾Ð±Ð¾Ñ€ Ð°Ñ€Ñ…Ð¸Ñ‚ÐµÐºÑ‚ÑƒÑ€Ð½Ñ‹Ñ… Ñ‡Ð°Ð½ÐºÐ¾Ð²
  for (const file of expandedFiles) {
    if (seedFiles.includes(file)) continue;

    const vector = await embedQueued(`imports exports ${file}`);
    const extraRaw = await index.queryItems(vector, 6);
    const extraCandidates = extraRaw.map((r) => ({
      id: r.item.id,
      vector: r.item.vector,
      score: r.score,
      text: r.item.content,
      source: r.item.metadata.source,
      section: r.item.metadata.section,
      estimatedTokens: r.item.metadata.estimatedTokens,
    }));
    const diversified = mmrSelect(extraCandidates, vector, 2, 0.6);

    for (const h of diversified) {
      h.graphScore = computeGraphScore(h, seedFiles);
      h.finalScore = h.score * h.graphScore;
      expandedHits.push(h);
    }
  }
  expandedHits.sort(
    (a, b) => (b.finalScore ?? b.score) - (a.finalScore ?? a.score)
  );
  // 5ï¸âƒ£ group by source
  const bySource = new Map();
  for (const h of expandedHits) {
    if (!bySource.has(h.source)) bySource.set(h.source, []);
    bySource.get(h.source).push(h);
  }

  // 6ï¸âƒ£ context assembly
  let used = 0;
  const ctx = [];

  for (const [, items] of bySource.entries()) {
    const ordered = [
      ...items.filter((i) => i.section === "__exports__"),
      ...items.filter((i) => i.section === "__imports__"),
      ...items.filter(
        (i) => i.section !== "__imports__" && i.section !== "__exports__"
      ),
    ];

    for (const h of ordered) {
      if (used + h.estimatedTokens > budget) {
        return ctx.join("\n\n");
      }
      ctx.push(h.text);
      used += h.estimatedTokens;
    }
  }

  return ctx.join("\n\n");
}

/* ===================== API ===================== */

export const docs = {
  search: async (query, topK) => search("web", query, topK),
  refresh_index: async () => {
    try {
      await refresh("web");

      return {
        content: [
          {
            type: "text",
            text: "Web docs index refreshed successfully",
          },
        ],
      };
    } catch (err) {
      return {
        content: [
          {
            type: "text",
            text:
              "Web docs index refresh failed:\n" +
              (String(err?.stack) || String(err?.message) || String(err)),
          },
        ],
      };
    }
  },
};

export const project = {
  search: async (query, topK) => search("project", query, topK),

  build_context: async (query, budget) =>
    buildContext("project", query, budget),

  refresh_index: async () => {
    try {
      await refresh("project");

      return {
        content: [
          {
            type: "text",
            text: "Project index refreshed successfully",
          },
        ],
      };
    } catch (err) {
      return {
        content: [
          {
            type: "text",
            text:
              "Project index refresh failed:\n" +
              (String(err?.stack) || String(err?.message) || String(err)),
          },
        ],
      };
    }
  },
};

/* ===================== AUTOREFRESH ===================== */
// const cfg = JSON.parse(await fs.readFile(ENGINES.project.sourcesFile, "utf8"));
// const roots = Object.values(cfg.local_recurses || {}).map((p) =>
//   path.resolve(ROOT, p.root)
// );
// watchProjectIndex(roots);

/* ðŸ”§ FIX: Ñ„Ð¸Ð»ÑŒÑ‚Ñ€Ð°Ñ†Ð¸Ñ Ð¿Ð¾ Ñ€Ð°ÑÑˆÐ¸Ñ€ÐµÐ½Ð¸ÑÐ¼ */
const VALID_EXT = /\.(js|ts|jsx|tsx)$/i;

function watchProjectIndex(rootPaths, debounceMs = 5000) {
  if (engineState.get("project") !== "ready") {
    throw new Error("watcher started before engine ready");
  }
  if (watcher) watcher.close();
  watcher = chokidar.watch(rootPaths, {
    ignored: /node_modules|\.git|\.docs-index/,
    persistent: true,
    ignoreInitial: true,
    awaitWriteFinish: {
      stabilityThreshold: 2000,
      pollInterval: 1000,
    },
  });

  let timer = null;

  const scheduleFileIndex = (filePath, event) => {
    if (!VALID_EXT.test(filePath)) return; // ðŸ”§ FIX

    if (timer) clearTimeout(timer);

    timer = setTimeout(async () => {
      const cfg = JSON.parse(
        await fs.readFile(ENGINES.project.sourcesFile, "utf8")
      );
      for (const p of Object.values(cfg.local_recurses || {})) {
        const absRoot = path.resolve(ROOT, p.root);
        const absFile = path.isAbsolute(filePath)
          ? filePath
          : path.resolve(ROOT, filePath);
        if (!absFile.startsWith(absRoot)) continue;
        const rel = path.relative(ROOT, absFile).replace(/\\/g, "/");

        if (event === "unlink") {
          await removeProjectFile(rel);
        } else {
          await indexProjectFile(rel, ROOT);
        }
      }
    }, debounceMs);
  };
  watcher
    .on("add", (p) => scheduleFileIndex(p, "add"))
    .on("change", (p) => scheduleFileIndex(p, "change"))
    .on("unlink", (p) => scheduleFileIndex(p, "unlink"));
}
